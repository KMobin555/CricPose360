{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9179757,"sourceType":"datasetVersion","datasetId":5548261},{"sourceId":9197811,"sourceType":"datasetVersion","datasetId":5560753}],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport shutil\nfrom sklearn.model_selection import train_test_split\n\n# Base directory containing the video folders\nbase_dir = '/kaggle/input/cric-shot-2-0/cricshot-cls-mp4'\nbase_dir = '/kaggle/input/cric-shot-yolo-pose-annotated/cricshot-yolo-ano'\n\noutput_dir = '/kaggle/working/cricshot-split'\n\n# Create output directories for train, val, and test sets\nsplits = ['train', 'val', 'test']\nfor split in splits:\n    os.makedirs(os.path.join(output_dir, split), exist_ok=True)\n\n# Define the split ratios\ntrain_ratio = 0.7\nval_ratio = 0.2\ntest_ratio = 0.1\n\n# Function to split files into train, val, and test\ndef split_data(class_dir, files, train_ratio, val_ratio):\n    train_files, temp_files = train_test_split(files, test_size=(1 - train_ratio))\n    val_files, test_files = train_test_split(temp_files, test_size=(test_ratio / (val_ratio + test_ratio)))\n    return train_files, val_files, test_files\n\n# Traverse through each class folder\nfor class_name in os.listdir(base_dir):\n    class_path = os.path.join(base_dir, class_name)\n    if os.path.isdir(class_path):\n        files = [f for f in os.listdir(class_path) if f.endswith('.mp4')]\n        \n        # Split the files\n        train_files, val_files, test_files = split_data(class_path, files, train_ratio, val_ratio)\n\n        # Copy files to corresponding folders\n        for split, split_files in zip(splits, [train_files, val_files, test_files]):\n            split_class_dir = os.path.join(output_dir, split, class_name)\n            os.makedirs(split_class_dir, exist_ok=True)\n            for file in split_files:\n                src = os.path.join(class_path, file)\n                dst = os.path.join(split_class_dir, file)\n                shutil.copyfile(src, dst)\n\nprint(\"Dataset successfully split into train, val, and test sets.\")\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-08-26T08:08:31.884796Z","iopub.execute_input":"2024-08-26T08:08:31.885130Z","iopub.status.idle":"2024-08-26T08:08:58.617455Z","shell.execute_reply.started":"2024-08-26T08:08:31.885106Z","shell.execute_reply":"2024-08-26T08:08:58.616394Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Dataset successfully split into train, val, and test sets.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\n\n# Update this path to point to your local cricshot-split directory\nsplit_dir = output_dir\n\n# Function to count files in each folder\ndef count_files_in_folders(base_dir):\n    folder_counts = {}\n    for split in os.listdir(base_dir):\n        split_path = os.path.join(base_dir, split)\n        if os.path.isdir(split_path):\n            folder_counts[split] = {}\n            for class_name in os.listdir(split_path):\n                class_path = os.path.join(split_path, class_name)\n                if os.path.isdir(class_path):\n                    num_files = len([f for f in os.listdir(class_path) if os.path.isfile(os.path.join(class_path, f))])\n                    folder_counts[split][class_name] = num_files\n    return folder_counts\n\n# Count the number of files in each folder\nfile_counts = count_files_in_folders(split_dir)\n\n# Print the results\nfor split, classes in file_counts.items():\n    print(f\"{split}:\")\n    for class_name, num_files in classes.items():\n        print(f\"  {class_name}: {num_files} files\")\n","metadata":{"execution":{"iopub.status.busy":"2024-08-26T08:08:58.619274Z","iopub.execute_input":"2024-08-26T08:08:58.619916Z","iopub.status.idle":"2024-08-26T08:08:58.646891Z","shell.execute_reply.started":"2024-08-26T08:08:58.619878Z","shell.execute_reply":"2024-08-26T08:08:58.645817Z"},"trusted":true},"outputs":[{"name":"stdout","text":"train:\n  square_cut: 139 files\n  lofted: 138 files\n  straight: 135 files\n  pull: 125 files\n  cover: 131 files\n  late_cut: 127 files\n  sweep: 135 files\n  flick: 126 files\n  defense: 134 files\n  hook: 126 files\nval:\n  square_cut: 40 files\n  lofted: 40 files\n  straight: 38 files\n  pull: 36 files\n  cover: 38 files\n  late_cut: 36 files\n  sweep: 39 files\n  flick: 36 files\n  defense: 38 files\n  hook: 36 files\ntest:\n  square_cut: 21 files\n  lofted: 20 files\n  straight: 20 files\n  pull: 18 files\n  cover: 19 files\n  late_cut: 19 files\n  sweep: 20 files\n  flick: 19 files\n  defense: 20 files\n  hook: 19 files\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import warnings\n\n# Suppress all warnings\nwarnings.filterwarnings(\"ignore\")\n","metadata":{"execution":{"iopub.status.busy":"2024-08-26T08:08:58.648272Z","iopub.execute_input":"2024-08-26T08:08:58.648606Z","iopub.status.idle":"2024-08-26T08:09:01.068243Z","shell.execute_reply.started":"2024-08-26T08:08:58.648582Z","shell.execute_reply":"2024-08-26T08:09:01.067141Z"},"trusted":true},"outputs":[],"execution_count":3},{"cell_type":"code","source":"!pip install -q ultralytics","metadata":{"execution":{"iopub.status.busy":"2024-08-26T08:09:01.070192Z","iopub.execute_input":"2024-08-26T08:09:01.070535Z","iopub.status.idle":"2024-08-26T08:09:15.708799Z","shell.execute_reply.started":"2024-08-26T08:09:01.070496Z","shell.execute_reply":"2024-08-26T08:09:15.707644Z"},"trusted":true},"outputs":[],"execution_count":4},{"cell_type":"code","source":"import cv2\nimport torch\nimport torch.nn as nn\nfrom torch import optim\nimport torchvision\nimport torchvision.transforms as transforms\nfrom torch.utils.data import Dataset, DataLoader\nimport os\n\n# Custom Dataset Class\nclass VideoDataset(Dataset):\n    def __init__(self, video_dir, transform=None, max_frames=16):\n        self.video_dir = video_dir\n        self.transform = transform\n        self.classes = sorted(os.listdir(video_dir))\n        self.video_files = []\n        self.max_frames = max_frames\n        for class_name in self.classes:\n            class_dir = os.path.join(video_dir, class_name)\n            for video in os.listdir(class_dir):\n                if video.endswith('.mp4'):\n                    self.video_files.append((os.path.join(class_dir, video), class_name))\n\n    def __len__(self):\n        return len(self.video_files)\n\n    def __getitem__(self, idx):\n        video_path, label = self.video_files[idx]\n        frames = self.load_video(video_path)\n        label_idx = self.classes.index(label)\n        \n        if self.transform:\n            frames = [self.transform(frame.float()) for frame in frames]  # Convert to float\n\n        # Stack frames to create a tensor of shape (C, T, H, W)\n        video_tensor = torch.stack(frames, dim=1)\n        \n        return video_tensor, label_idx\n    \n    def load_video(self, video_path):\n        cap = cv2.VideoCapture(video_path)\n        \n        frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n        interval = max(1, frame_count // self.max_frames)  # Ensure interval is at least 1\n        frames = []\n        \n        for i in range(self.max_frames):\n            cap.set(cv2.CAP_PROP_POS_FRAMES, i * interval)\n            ret, frame = cap.read()\n            if not ret:\n                break\n            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n            frame = torch.from_numpy(frame).permute(2, 0, 1)  # Convert to (C, H, W)\n            frames.append(frame)\n            \n        cap.release()\n        return frames\n\n# Transforms\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.RandomHorizontalFlip(),\n    transforms.Normalize((0.5,), (0.5,))\n])\n\n# video_path = os.path.join(category_path, video_file)\n#             cap = cv2.VideoCapture(video_path)\n#             frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n#             interval = max(1, frame_count // num_frames)  # Ensure interval is at least 1\n\n#             for i in range(num_frames):\n#                 cap.set(cv2.CAP_PROP_POS_FRAMES, i * interval)\n#                 ret, frame = cap.read()\n#                 if not ret:\n#                     break\n#                 resized_frame = cv2.resize(frame, img_size)\n#                 frame_filename = os.path.join(output_category_path, f\"{os.path.splitext(video_file)[0]}_frame_{i}.jpg\")\n#                 cv2.imwrite(frame_filename, resized_frame)\n#             cap.release()","metadata":{"execution":{"iopub.status.busy":"2024-08-26T08:11:31.584535Z","iopub.execute_input":"2024-08-26T08:11:31.585452Z","iopub.status.idle":"2024-08-26T08:11:31.601595Z","shell.execute_reply.started":"2024-08-26T08:11:31.585414Z","shell.execute_reply":"2024-08-26T08:11:31.600785Z"},"trusted":true},"outputs":[],"execution_count":15},{"cell_type":"code","source":"# Paths to train, val, test directories\ntrain_dir = output_dir+'/train'\nval_dir = output_dir+'/val'\ntest_dir = output_dir+'/test'\n\n# Datasets and DataLoaders\ntrain_dataset = VideoDataset(train_dir, transform=transform)\nval_dataset = VideoDataset(val_dir, transform=transform)\ntest_dataset = VideoDataset(test_dir, transform=transform)\n\nbatch_size = 2\n\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\nval_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-26T08:11:31.603240Z","iopub.execute_input":"2024-08-26T08:11:31.603564Z","iopub.status.idle":"2024-08-26T08:11:31.624241Z","shell.execute_reply.started":"2024-08-26T08:11:31.603539Z","shell.execute_reply":"2024-08-26T08:11:31.623356Z"},"trusted":true},"outputs":[],"execution_count":16},{"cell_type":"code","source":"len(train_dataset)","metadata":{"execution":{"iopub.status.busy":"2024-08-26T08:11:31.780762Z","iopub.execute_input":"2024-08-26T08:11:31.781105Z","iopub.status.idle":"2024-08-26T08:11:31.787068Z","shell.execute_reply.started":"2024-08-26T08:11:31.781073Z","shell.execute_reply":"2024-08-26T08:11:31.786214Z"},"trusted":true},"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"1316"},"metadata":{}}],"execution_count":17},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision import transforms\nimport torch.nn.functional as F\nimport torchvision.models as models\nimport os\nimport cv2\nimport numpy as np\nfrom tqdm import tqdm\n\n# Block for r3d_34\nclass BasicBlock3D(nn.Module):\n    expansion = 1\n\n    def __init__(self, in_planes, planes, stride=1, downsample=None):\n        super(BasicBlock3D, self).__init__()\n        self.conv1 = nn.Conv3d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm3d(planes)\n        self.conv2 = nn.Conv3d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm3d(planes)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n\n    def forward(self, x):\n        identity = x\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            identity = self.downsample(x)\n\n        out += identity\n        out = self.relu(out)\n\n        return out\n\nclass Bottleneck3D(nn.Module):\n    expansion = 4\n\n    def __init__(self, in_planes, planes, stride=1, downsample=None):\n        super(Bottleneck3D, self).__init__()\n        self.conv1 = nn.Conv3d(in_planes, planes, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm3d(planes)\n        self.conv2 = nn.Conv3d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm3d(planes)\n        self.conv3 = nn.Conv3d(planes, planes * self.expansion, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm3d(planes * self.expansion)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n\n    def forward(self, x):\n        identity = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            identity = self.downsample(x)\n\n        out += identity\n        out = self.relu(out)\n\n        return out\n\nclass ResNet3D(nn.Module):\n    def __init__(self, block, layers, num_classes=400):\n        super(ResNet3D, self).__init__()\n        self.in_planes = 64\n\n        self.conv1 = nn.Conv3d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n        self.bn1 = nn.BatchNorm3d(64)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool3d(kernel_size=3, stride=2, padding=1)\n\n        self.layer1 = self._make_layer(block, 64, layers[0])\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n\n        self.avgpool = nn.AdaptiveAvgPool3d((1, 1, 1))\n        self.fc = nn.Linear(512 * block.expansion, num_classes)\n\n    def _make_layer(self, block, planes, blocks, stride=1):\n        downsample = None\n        if stride != 1 or self.in_planes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv3d(self.in_planes, planes * block.expansion, kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm3d(planes * block.expansion),\n            )\n\n        layers = []\n        layers.append(block(self.in_planes, planes, stride, downsample))\n        self.in_planes = planes * block.expansion\n        for _ in range(1, blocks):\n            layers.append(block(self.in_planes, planes))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n\n        x = self.avgpool(x)\n        x = torch.flatten(x, 1)\n        x = self.fc(x)\n\n        return x\n\ndef r3d_34(num_classes=400):\n    return ResNet3D(BasicBlock3D, [3, 4, 6, 3], num_classes)\n\ndef r3d_50(num_classes=400):\n    return ResNet3D(Bottleneck3D, [3, 4, 6, 3], num_classes)\n\n# Load the custom ResNet-3D models\nnum_classes = 10  # Set the number of classes according to your dataset\nmodel_34 = r3d_34(num_classes=num_classes)\nmodel_50 = r3d_50(num_classes=num_classes)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-26T08:11:32.149530Z","iopub.execute_input":"2024-08-26T08:11:32.150175Z","iopub.status.idle":"2024-08-26T08:11:33.209115Z","shell.execute_reply.started":"2024-08-26T08:11:32.150142Z","shell.execute_reply":"2024-08-26T08:11:33.208189Z"},"trusted":true},"outputs":[],"execution_count":18},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision.models.video import r3d_18\nimport copy\n\n# Load the pre-trained ResNet-3D model\nmodel = r3d_18(pretrained=False)\n\n# Modify the final layer for your number of classes\nnum_classes = len(train_dataset.classes)  # Number of classes in your dataset\nmodel.fc = nn.Linear(model.fc.in_features, num_classes)\n\n# Move model to GPU if available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)\n\ncriterion = nn.CrossEntropyLoss()  # For multi-class classification\noptimizer = optim.Adam(model.parameters(), lr=1e-4)\n\n# Training, validation, and testing\nnum_epochs = 30\nbest_val_loss = float('inf')\nbest_accuracy = 0.0\nbest_model_wts = \nprnt_aftr = 100\n\nno_imp = 0\nepoch_limit = 7\n\nfor epoch in range(num_epochs):\n    print(f'Epoch {epoch+1}/{num_epochs}')\n    print('-' * 50)\n\n    # Training phase\n    model.train()\n    running_loss = 0.0\n    for i, (inputs, labels) in enumerate(train_loader):\n        inputs, labels = inputs.float().to(device), labels.to(device)\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item()\n\n        if i % prnt_aftr == 0:\n            print(f'[Training] Iteration {i}/{len(train_loader)} Loss: {loss.item():.4f}')\n\n    epoch_train_loss = running_loss / len(train_loader)\n    print(f'Training Loss: {epoch_train_loss:.4f}')\n\n    # Validation phase\n    model.eval()\n    val_loss = 0.0\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for i, (inputs, labels) in enumerate(val_loader):\n            inputs, labels = inputs.float().to(device), labels.to(device)\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            val_loss += loss.item()\n            _, predicted = torch.max(outputs, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n\n            if i % prnt_aftr == 0:\n                print(f'[Validation] Iteration {i}/{len(val_loader)} Loss: {loss.item():.4f}')\n\n    epoch_val_loss = val_loss / len(val_loader)\n    print(f'Validation Loss: {epoch_val_loss:.4f}')\n    val_accuracy = 100 * correct / total\n    print(f'Validation Accuracy: {val_accuracy}%')\n\n#     # Save the best model\n#     if epoch_val_loss < best_val_loss:\n#         best_val_loss = epoch_val_loss\n#         best_model_wts = copy.deepcopy(model.state_dict())\n#         torch.save(model.state_dict(), f'best_model_epoch_{epoch+1}.pth')\n#         print(f'Saved best model for epoch {epoch+1}')\n    if no_imp > epoch_limit:\n        print(f\"No improvement for {no_imp} epoch, so terminating\")\n        break\n    no_imp+=1\n    # Save the best model\n    if val_accuracy > best_accuracy:\n        best_accuracy = val_accuracy\n        best_model_wts = copy.deepcopy(model.state_dict())\n        no_imp = 0\n        torch.save(model.state_dict(), f'best_model_epoch_{epoch+1}.pth')\n        print(f'Saved best model for epoch {epoch+1}')\n        print(f\"New best model saved with accuracy: {best_accuracy}%\")\n","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-08-26T08:12:30.696465Z","iopub.execute_input":"2024-08-26T08:12:30.697189Z","iopub.status.idle":"2024-08-26T08:12:41.097910Z","shell.execute_reply.started":"2024-08-26T08:12:30.697157Z","shell.execute_reply":"2024-08-26T08:12:41.096395Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Epoch 1/30\n--------------------------------------------------\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[20], line 41\u001b[0m\n\u001b[1;32m     39\u001b[0m inputs, labels \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     40\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 41\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m     43\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchvision/models/video/resnet.py:261\u001b[0m, in \u001b[0;36mVideoResNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[38;5;66;03m# Flatten the layer to fc\u001b[39;00m\n\u001b[1;32m    260\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mflatten(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 261\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mRuntimeError\u001b[0m: CUDA error: CUBLAS_STATUS_ALLOC_FAILED when calling `cublasCreate(handle)`"],"ename":"RuntimeError","evalue":"CUDA error: CUBLAS_STATUS_ALLOC_FAILED when calling `cublasCreate(handle)`","output_type":"error"}],"execution_count":20},{"cell_type":"code","source":"import torch\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# Load best model weights\nmodel.load_state_dict(best_model_wts)\n\n# Testing phase\nmodel.eval()\ntest_loss = 0.0\ncorrect = 0\ntotal = 0\nall_labels = []\nall_preds = []\nincorrect_count = 0  # To count incorrect predictions\n\nwith torch.no_grad():\n    for inputs, labels in test_loader:\n        inputs, labels = inputs.float().to(device), labels.to(device)\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        test_loss += loss.item()\n        _, predicted = torch.max(outputs, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n        all_labels.extend(labels.cpu().numpy())\n        all_preds.extend(predicted.cpu().numpy())\n        \n        # Count incorrect predictions\n        incorrect_count += (predicted != labels).sum().item()\n\ntest_loss /= len(test_loader)\naccuracy = 100 * correct / total\n\nprint(f\"Test Accuracy: {accuracy:.2f}%\")\nprint(f\"Incorrect Predictions: {incorrect_count} out of {total}\")\nprint(f\"Test Loss: {test_loss:.4f}\")\n\n# Convert to numpy arrays\nall_labels = np.array(all_labels)\nall_preds = np.array(all_preds)\n\n# Save accuracy and loss to a text file\nwith open(\"test_results.txt\", \"w\") as f:\n    f.write(f'Test Loss: {test_loss:.4f}\\n')\n    f.write(f'Test Accuracy: {accuracy:.2f}%\\n')\n    f.write(f'Incorrect Predictions: {incorrect_count} out of {total}\\n')\n\n# Classification report\nreport = classification_report(all_labels, all_preds, target_names=train_dataset.classes)\nprint(\"Classification Report:\\n\", report)  # Display before saving\nwith open(\"classification_report.txt\", \"w\") as f:\n    f.write(\"Classification Report:\\n\")\n    f.write(report)\n\nprint(report)\n\n# Confusion matrix\nconf_matrix = confusion_matrix(all_labels, all_preds, labels=list(range(num_classes)))\n\n# Save confusion matrix as a CSV file\nconf_matrix_df = pd.DataFrame(conf_matrix, index=train_dataset.classes, columns=train_dataset.classes)\nconf_matrix_df.to_csv(\"confusion_matrix.csv\")\n\n# Plot and save the confusion matrix as an image\nplt.figure(figsize=(10, 7))\nsns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=train_dataset.classes, yticklabels=train_dataset.classes)\nplt.xlabel('Predicted Label')\nplt.ylabel('True Label')\nplt.title('Confusion Matrix')\nplt.savefig(\"confusion_matrix.png\")\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-08-26T08:09:57.758805Z","iopub.status.idle":"2024-08-26T08:09:57.759183Z","shell.execute_reply.started":"2024-08-26T08:09:57.759007Z","shell.execute_reply":"2024-08-26T08:09:57.759023Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(all_labels),len(all_preds)","metadata":{"execution":{"iopub.status.busy":"2024-08-26T08:09:57.760448Z","iopub.status.idle":"2024-08-26T08:09:57.760764Z","shell.execute_reply.started":"2024-08-26T08:09:57.760611Z","shell.execute_reply":"2024-08-26T08:09:57.760624Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null}]}