{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9398467,"sourceType":"datasetVersion","datasetId":5704538},{"sourceId":114997,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":96581,"modelId":120764},{"sourceId":115308,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":96850,"modelId":121034}],"dockerImageVersionId":30762,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\"\"\"\nTraining with 3 epochs\nwith yolo-pose-ano 3\non yolov8l pose\n\n\"\"\"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install -q ultralytics opencv-python scikit-image\n!pip install -q -U ipywidgets","metadata":{"_uuid":"d1c4b959-a698-4c4b-8ba0-1fd9c5d56262","_cell_guid":"5d506941-6979-4e67-be98-ae33d2bd6dd9","collapsed":false,"scrolled":true,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-09-14T17:28:03.516893Z","iopub.execute_input":"2024-09-14T17:28:03.517289Z","iopub.status.idle":"2024-09-14T17:28:37.245889Z","shell.execute_reply.started":"2024-09-14T17:28:03.517253Z","shell.execute_reply":"2024-09-14T17:28:37.244452Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import os\n# import shutil\n# from sklearn.model_selection import train_test_split\n\n# # Base directory containing the video folders\n# base_dir = '/kaggle/input/yolo-pose-ano-3/cricshot-yolo-ano'\n# base_dir = '/kaggle/input/cric-pose-v2/cric_pose_v2'\n\n# output_dir = '/kaggle/working/cricshot-split'\n\n# # Create output directories for train, val, and test sets\n# splits = ['train', 'val', 'test']\n# for split in splits:\n#     os.makedirs(os.path.join(output_dir, split), exist_ok=True)\n\n# # Define the split ratios\n# train_ratio = 0.7\n# val_ratio = 0.2\n# test_ratio = 0.1\n\n# # Function to split files into train, val, and test\n# def split_data(class_dir, files, train_ratio, val_ratio):\n#     train_files, temp_files = train_test_split(files, test_size=(1 - train_ratio))\n#     val_files, test_files = train_test_split(temp_files, test_size=(test_ratio / (val_ratio + test_ratio)))\n#     return train_files, val_files, test_files\n\n# # Traverse through each class folder\n# for class_name in os.listdir(base_dir):\n#     class_path = os.path.join(base_dir, class_name)\n#     if os.path.isdir(class_path):\n#         files = [f for f in os.listdir(class_path) if f.endswith('.mp4')]\n        \n#         # Split the files\n#         train_files, val_files, test_files = split_data(class_path, files, train_ratio, val_ratio)\n\n#         # Copy files to corresponding folders\n#         for split, split_files in zip(splits, [train_files, val_files, test_files]):\n#             split_class_dir = os.path.join(output_dir, split, class_name)\n#             os.makedirs(split_class_dir, exist_ok=True)\n#             for file in split_files:\n#                 src = os.path.join(class_path, file)\n#                 dst = os.path.join(split_class_dir, file)\n#                 shutil.copyfile(src, dst)\n\n# print(\"Dataset successfully split into train, val, and test sets.\")","metadata":{"_uuid":"afd8854e-d77a-4473-a70c-cf19ad5ec618","_cell_guid":"10e73b55-018b-4e59-9a05-112edadca7e6","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-09-14T17:28:37.248437Z","iopub.execute_input":"2024-09-14T17:28:37.249416Z","iopub.status.idle":"2024-09-14T17:29:21.632801Z","shell.execute_reply.started":"2024-09-14T17:28:37.249366Z","shell.execute_reply":"2024-09-14T17:29:21.631709Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import os\n# import cv2\n# import numpy as np\n# from skimage import io\n# from tqdm import tqdm\n\n# # Define paths\n# dataset_dir = output_dir\n# processed_data_dir = \"processed_data\"\n# os.makedirs(processed_data_dir, exist_ok=True)\n\n# # Function to extract and save frames from videos\n# def process_videos(directory, output_dir, num_frames=32, img_size=(640, 640)):\n#     categories = os.listdir(directory)\n    \n#     for category in categories:\n#         category_path = os.path.join(directory, category)\n#         output_category_path = os.path.join(output_dir, category)\n#         os.makedirs(output_category_path, exist_ok=True)\n        \n#         video_files = os.listdir(category_path)\n#         for video_file in tqdm(video_files, desc=f\"Processing videos in {category}\", leave=True):\n#             video_path = os.path.join(category_path, video_file)\n#             cap = cv2.VideoCapture(video_path)\n#             frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n#             interval = max(1, frame_count // num_frames)  # Ensure interval is at least 1\n\n#             for i in range(num_frames):\n#                 cap.set(cv2.CAP_PROP_POS_FRAMES, i * interval)\n#                 ret, frame = cap.read()\n#                 if not ret:\n#                     break\n#                 resized_frame = cv2.resize(frame, img_size)\n#                 frame_filename = os.path.join(output_category_path, f\"{os.path.splitext(video_file)[0]}_frame_{i}.jpg\")\n#                 cv2.imwrite(frame_filename, resized_frame)\n#             cap.release()\n\n# # Process train, val, and optionally test sets\n# process_videos(os.path.join(dataset_dir, \"train\"), os.path.join(processed_data_dir, \"train\"))\n# process_videos(os.path.join(dataset_dir, \"val\"), os.path.join(processed_data_dir, \"val\"))\n# process_videos(os.path.join(dataset_dir, \"test\"), os.path.join(processed_data_dir, \"test\"))","metadata":{"_uuid":"2821848f-c48d-422d-a5ba-7791b4a81a12","_cell_guid":"35de3a39-52a2-4f63-87ee-901bbcd20b72","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-09-14T17:29:21.634983Z","iopub.execute_input":"2024-09-14T17:29:21.635754Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!cp -r /kaggle/input/trainable-yolo-pose-3/* /kaggle/working/","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data_yaml_content = \"\"\"\npath: /kaggle/working/processed_data  \ntrain: train  \nval: val     \ntest: test\nnc: 10     \nnames:\n  0: 'sweep'\n  1: 'square_cut'\n  2: 'hook'\n  3: 'lofted'\n  4: 'cover'\n  5: 'late_cut'\n  6: 'pull'\n  7: 'defense'\n  8: 'flick'\n  9: 'straight'\n\n\"\"\"\n\n\n# Path where you want to save the data.yaml file\nyaml_file_path = '/kaggle/working/data.yaml'\n\n# Save the YAML content to the file\nwith open(yaml_file_path, 'w') as file:\n    file.write(data_yaml_content)\n\n# Confirm the file has been saved\nprint(f\"data.yaml saved at: {yaml_file_path}\")","metadata":{"_uuid":"025b177c-29af-4e9e-bced-1b3b6ccf3d33","_cell_guid":"e4980e8c-5071-497d-9e1c-38a9ba629e2d","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Read the YAML file content (optional)\nwith open(yaml_file_path, 'r') as file:\n    content = file.read()\n    print(content)","metadata":{"_uuid":"36199331-c6d9-4a1d-a401-52d38e9b2b09","_cell_guid":"ff6f3ae6-a9a3-4ae1-adab-ea45e5d6f1e3","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Initialize Weights & Biases (W&B) in disabled mode.\n\nimport wandb\nwandb.init(mode=\"disabled\")","metadata":{"_uuid":"3a96070d-294f-41c9-a99d-ab4bab9014ff","_cell_guid":"fac61d4d-5a24-44c5-b19b-707d246e79c1","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from ultralytics import YOLO\n\ndata_file = '/kaggle/working/processed_data/'\n# data_file = yaml_file_path\n\n# Load and train the YOLOv8 model using the data.yaml file\nmodel = YOLO('/kaggle/input/ano-3rd-49-eps/pytorch/default/1/best NA 49.pt')\n# model = YOLO('yolov8l-cls.pt') \n\nmodel.train(\n    data=data_file, \n    epochs=15, \n    imgsz=640\n)","metadata":{"_uuid":"d263dc5a-bf9c-42de-9728-97fbb6711337","_cell_guid":"9e337733-ae45-437e-9fb3-db74bc60cb27","collapsed":false,"scrolled":true,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"_uuid":"2e4ed21d-6862-47ed-9cac-5fa2dc0e0f93","_cell_guid":"a0c73ac4-aa13-4f32-a21c-235b5f3eb050","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport os\n\n# Base directory for the test videos\ntest_base_dir = '/kaggle/working/cricshot-frames/test'  # Update this path to where your test videos are stored\ntest_base_dir = 'cricshot-split/test'\n\n# Prepare the ground truth DataFrame by scanning the test directory\nground_truth_data = []\nfor class_name in os.listdir(test_base_dir):\n    class_dir = os.path.join(test_base_dir, class_name)\n    if os.path.isdir(class_dir):\n        for video_name in os.listdir(class_dir):\n            if video_name.endswith('.mp4'):  # Assuming the videos are in .mp4 format\n                ground_truth_data.append({\n                    'video_name': video_name,\n                    'true_class': class_name\n                })\n\n# Convert to a DataFrame\nground_truth_df = pd.DataFrame(ground_truth_data)\nprint(ground_truth_df)","metadata":{"_uuid":"20308392-78d8-4899-a415-32b9fc76271b","_cell_guid":"66c82c22-fb4c-46ef-871a-c45d4fdb585f","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import cv2\nimport os\nimport pandas as pd\nfrom ultralytics import YOLO\nfrom collections import Counter, defaultdict\n\nsaved_model_path = '/kaggle/input/v2-yolov8l-cls/pytorch/default/1/best (9).pt'\nsaved_model_path = '/kaggle/working/runs/classify/train/weights/best.pt'\n\n# Load the trained YOLOv8 model\nmodel = YOLO(saved_model_path)  # Update to your trained model path\n\n# Initialize a list to store predictions\npredictions = []\n\n# Function to extract exactly 32 frames from a video\ndef extract_frames(video_path, num_frames=32, img_size=(640, 640)):\n    cap = cv2.VideoCapture(video_path)\n    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n    if frame_count < num_frames:\n        raise ValueError(f\"Video {video_path} has fewer frames ({frame_count}) than requested ({num_frames}).\")\n    \n    interval = max(1, frame_count // num_frames)\n    frames = []\n    for i in range(num_frames):\n        cap.set(cv2.CAP_PROP_POS_FRAMES, i * interval)\n        ret, frame = cap.read()\n        if not ret:\n            break\n        resized_frame = cv2.resize(frame, img_size)\n        frames.append(resized_frame)\n    cap.release()\n    return frames\n\n# Loop through the test directories and videos\nfor class_name in os.listdir(test_base_dir):\n    class_dir = os.path.join(test_base_dir, class_name)\n    if os.path.isdir(class_dir):\n        for video_name in os.listdir(class_dir):\n            if video_name.endswith('.mp4'):\n                video_path = os.path.join(class_dir, video_name)\n                \n                try:\n                    # Extract 32 frames from the video\n                    frames = extract_frames(video_path, num_frames=32)\n                    \n                    # Predict on video frames\n                    results = model.predict(frames)\n#                     print(results[0].probs.top1)\n                    # Initialize a dictionary to accumulate class probabilities\n                    class_probabilities = defaultdict(list)\n\n                    # Collect frame-level probabilities\n                    for result in results:\n                        top_class = result.names[result.probs.top1]\n                        top_prob = result.probs.top1conf.item()\n                        class_probabilities[top_class].append(top_prob)\n                    \n                    # Calculate average probability for each class\n                    avg_class_probabilities = {cls: sum(probs) / len(results) for cls, probs in class_probabilities.items()}\n                    \n                    print(avg_class_probabilities)\n                    \n                    # Determine the class with the highest average probability\n                    most_probable_class = max(avg_class_probabilities, key=avg_class_probabilities.get)\n                    \n                    print(f\"Video: {video_name}, Most Probable Class: {most_probable_class}, Avg Probability: {avg_class_probabilities[most_probable_class]:.4f}\")\n                    \n                    # Store the prediction\n                    predictions.append({\n                        'video_name': video_name,\n                        'true_class': class_name,\n                        'predicted_class': most_probable_class,\n                        'avg_probability': avg_class_probabilities[most_probable_class]\n                    })\n                except ValueError as e:\n                    print(f\"Skipping {video_path}: {e}\")\n                    \n                    \n# Convert predictions to a DataFrame\npredictions_df = pd.DataFrame(predictions)\nprint(predictions_df)\n\n# Example: Print the predicted class for the first video\nif not predictions_df.empty:\n    first_prediction = predictions_df.iloc[0]\n    print(f\"Video: {first_prediction['video_name']}, Predicted Class: {first_prediction['predicted_class']}\")","metadata":{"_uuid":"52ff062f-0cb5-4bf1-9e50-30cba9b243ef","_cell_guid":"5be9da62-57ab-411a-96e4-b879598bfd05","collapsed":false,"scrolled":true,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Merge predictions with ground truth for comparison\ncomparison_df = predictions_df\n\n# comparison_df.to_csv(\"comperision_df.csv\",index=False)\n# Display the comparison DataFrame\ncomparison_df","metadata":{"_uuid":"26b9411c-6194-4174-bd35-8ad4059968aa","_cell_guid":"5d4a26ae-162e-4681-a655-3e77007b3220","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport pandas as pd\nfrom sklearn.metrics import confusion_matrix, classification_report, accuracy_score\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Assume comparison_df is already created\n\n# Calculate accuracy\naccuracy = accuracy_score(comparison_df['true_class'], comparison_df['predicted_class'])\naccuracy_percentage = accuracy * 100\naccuracy_formatted = f\"{accuracy:.6f}\"\naccuracy_percentage_formatted = f\"{accuracy_percentage:.4f}%\"\nprint(f\"Accuracy: {accuracy_formatted} ({accuracy_percentage_formatted})\")\n\n# Generate classification report\nclass_report = classification_report(comparison_df['true_class'], comparison_df['predicted_class'], target_names=os.listdir(test_base_dir), digits=4)\nprint(\"Classification Report:\\n\", class_report)\n\n# Generate confusion matrix\ncm = confusion_matrix(comparison_df['true_class'], comparison_df['predicted_class'], labels=os.listdir(test_base_dir))\n\n# Plot confusion matrix\nplt.figure(figsize=(10, 8))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=os.listdir(test_base_dir), yticklabels=os.listdir(test_base_dir))\nplt.xlabel('Predicted')\nplt.ylabel('True')\nplt.title('Confusion Matrix')\nplt.savefig('confusion_matrix.png')  # Save as PNG\nplt.show()\nplt.close()  # Close the plot to free memory\n\n# Calculate classification errors\ncomparison_df['error'] = comparison_df['true_class'] != comparison_df['predicted_class']\nerror_count = comparison_df['error'].sum()\ntotal_count = len(comparison_df)\nerror_rate = error_count / total_count\nerror_rate_percentage = error_rate * 100\nerror_rate_formatted = f\"{error_rate:.4f}\"\nerror_rate_percentage_formatted = f\"{error_rate_percentage:.2f}%\"\n\nprint(f\"Total classification errors: {error_count}\")\nprint(f\"Error rate: {error_rate_formatted} ({error_rate_percentage_formatted})\")\n\n# Export the results\n# 1. Save accuracy to a text file\nwith open('accuracy.txt', 'w') as f:\n    f.write(f\"Accuracy: {accuracy_formatted} ({accuracy_percentage_formatted})\\n\")\n\n# 2. Save classification report to a text file\nwith open('classification_report.txt', 'w') as f:\n    f.write(\"Classification Report:\\n\")\n    f.write(class_report)\n\n# 3. Save the comparison DataFrame including errors to a CSV file\ncomparison_df.to_csv('classification_errors.csv', index=False)\n\n# 4. Save error summary to a text file\nwith open('error_summary.txt', 'w') as f:\n    f.write(f\"Total classification errors: {error_count}\\n\")\n    f.write(f\"Error rate: {error_rate_formatted} ({error_rate_percentage_formatted})\\n\")\n","metadata":{"_uuid":"64fe9b06-a665-4235-969b-fec47bf4be1d","_cell_guid":"c721f6e1-5022-4b1f-8b36-01a89c026a84","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"_uuid":"08783b7a-baee-4638-ab3b-ae569f5e7c0a","_cell_guid":"429ff12d-11d6-47c7-af23-37010c47830a","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"execution_count":null}]}