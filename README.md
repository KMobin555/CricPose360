# CricPose360: Real-Time Pose-Enhanced Cricket Shot Classification

## 🌟 **Overview**

CricPose360 is a dataset and model pipeline designed for **real-time cricket shot classification**. This project introduces a novel **pose-enhanced approach** that improves classification accuracy by utilizing **skeleton keypoints** extracted from videos using the **YOLOv8 model**. The CricPose360 dataset consists of **3,030 annotated cricket videos**, featuring **10 distinct cricket shots**.

---

## 📂 **Project Structure**

The project is organized into the following key components:

```bash
/CricPose360_Project
│
├── EDA.ipynb                     # Exploratory Data Analysis of the dataset
├── LICENSE                       # Project license information
├── README.md                     # Project overview and instructions
├── Resnet3Dl_model_training.ipynb # Training script for ResNet3D model
├── Video-processing.ipynb        # Video preprocessing script
├── YOLOV8-cls_fine_tuning.ipynb  # Fine-tuning YOLOv8 for classification
├── YOLOV8_Inference(fine_tuned).ipynb # Inference using fine-tuned YOLOv8 model
```

## 🚀 Installation

Ensure you have Python 3.8+ installed, along with the required dependencies. You can install them via pip:

```bash
pip install -r requirements.txt
``` 

## 🎥 **Dataset**

The CricPose360 dataset consists of **3,030 annotated cricket videos**, divided into **10 distinct cricket shots**:

1. **Cover Drive**
2. **Defensive Shot**
3. **Flick**
4. **Hook**
5. **Late Cut**
6. **Lofted Shot**
7. **Pull Shot**
8. **Square Cut**
9. **Straight Drive**
10. **Sweep Shot**

The dataset is split into:
- **Training Set**: 70%
- **Validation Set**: 20%
- **Test Set**: 10%


## 🏃‍♂️ **Pose-Enhanced Data**

Pose-enhanced data is generated by extracting **skeleton keypoints** from each video frame using the **YOLOv8 pose model**. These pose-enhanced videos are then used to train a **YOLOv8 model** for shot classification.

<div style="display: flex; align-items: center; justify-content: center;">
  <div style="text-align: center;">
    <p>Normal Video</p>
    <img src="./images/flick_0015_norm.gif" alt="Gif 1" width="300" style="margin-right: 10px;">
  </div>
  <span style="font-size: 50px; margin-left: 10px; margin-right: 10px;">→</span>
  <div style="text-align: center;">
    <p>Pose-Enhanced Video</p>
    <img src="./images/flick_0015_pose.gif" alt="Gif 2" width="300">
  </div>
</div>


-----
-----

<div style="display: flex; align-items: center; justify-content: center;">
  <div style="text-align: center;">
    <p>Normal Video</p>
    <img src="./images/sweep_0002_norm.gif" alt="Gif 1" width="300" style="margin-right: 10px;">
  </div>
  <span style="font-size: 50px; margin-left: 10px; margin-right: 10px;">→</span>
  <div style="text-align: center;">
    <p>Pose-Enhanced Video</p>
    <img src="./images/sweep_0002_pose.gif" alt="Gif 2" width="300">
  </div>
</div>

---

## 🔬 **Methodology**

### **1. Data Collection**
- 3,030 cricket videos, each containing 3-7 second clips of various batting shots.

### **2. Preprocessing**
- Extract frames from the videos.
- Generate pose annotations using the **YOLOv8 pose model**.

### **3. Model Training**
- Fine-tune the **YOLOv8 model** on the pose-enhanced dataset for shot classification.

### **4. Inference**
- Use the fine-tuned **YOLOv8 model** for classifying unseen cricket shots.

---

## 📊 **Results**

- **Accuracy**: 92.21% on the test dataset.
- **F1 Score**: 92.29%.
- **Inference Speed**: 16.1 ms/frame.

---

## 🚀 **Future Work**

- **Optimizing pose-enhanced data** for even higher accuracy.
- Exploring **transformer-based models** for further classification enhancement.
- Integrating **ball tracking techniques** to improve shot classification.

---

## 📝 **License**

This project is licensed under the **MIT License**. See the `LICENSE` file for more details.

---

## 🙏 **Acknowledgements**

- **YOLOv8**: For object detection and pose estimation.
- **FFmpeg**: For video frame extraction and reconstruction.
